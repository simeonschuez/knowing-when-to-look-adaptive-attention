{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'caption data/TRAIN_IMAGES_coco.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-bcb9a1aa4f89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    253\u001b[0m                                      std=[0.229, 0.224, 0.225])\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m     train_loader = torch.utils.data.DataLoader(COCOTrainDataset(transform=transforms.Compose([normalize])),\n\u001b[0m\u001b[1;32m    256\u001b[0m                                                \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m                                                \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dokumente/Projekte/reg/knowing-when-to-look-adaptive-attention/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, transform)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Open hdf5 file where images are stored\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'caption data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TRAIN_IMAGES_coco'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/local/.anaconda/envs/spacy/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/local/.anaconda/envs/spacy/lib/python3.6/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'caption data/TRAIN_IMAGES_coco.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from cococaption.pycocotools.coco import COCO\n",
    "from cococaption.pycocoevalcap.eval import COCOEvalCap\n",
    "import torch.backends.cudnn as cudnn\n",
    "from models import *\n",
    "from util import *\n",
    "from dataset import *\n",
    "\n",
    "#Model Parameters\n",
    "emb_dim = 512                  # dimension of word embeddings\n",
    "attention_dim = 512            # attention hidden size\n",
    "hidden_size = 512              # dimension of decoder RNN\n",
    "cudnn.benchmark = True         # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
    "# Training parameters\n",
    "start_epoch = 0\n",
    "epochs = 40                             # number of epochs to train before finetuning the encoder. Set to 18 when finetuning ecoder\n",
    "epochs_since_improvement = 0            # keeps track of number of epochs since there's been an improvement in validation BLEU\n",
    "batch_size = 80                         # set to 32 when finetuning the encoder\n",
    "workers = 1                             # number of workers for data-loading\n",
    "encoder_lr = 1e-4                       # learning rate for encoder. if fine-tuning, change to 1e-5 for CNN parameters only\n",
    "decoder_lr = 5e-4                       # learning rate for decoder\n",
    "grad_clip = 0.1                         # clip gradients at an absolute value of\n",
    "best_cider = 0.                         # Current BLEU-4 score \n",
    "print_freq = 100                        # print training/validation stats every __ batches\n",
    "fine_tune_encoder = False                # set to true after 20 epochs \n",
    "checkpoint = None    # path to checkpoint, None at the begining\n",
    "annFile = 'cococaptioncider/annotations/captions_val2014.json'  # Location of validation annotations\n",
    "\n",
    "\n",
    "def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch, vocab_size):\n",
    "\n",
    "    decoder.train()                 # train mode (dropout and batchnorm is used)\n",
    "    encoder.train()\n",
    "    losses = AverageMeter()         # loss (per decoded word)\n",
    "    top5accs = AverageMeter()       # top5 accuracy\n",
    "\n",
    "    # Batches\n",
    "    for i, (imgs, caps, caplens) in enumerate(train_loader):\n",
    "\n",
    "        # Move to GPU, if available\n",
    "        imgs = imgs.to(device)\n",
    "        caps = caps.to(device)\n",
    "        caplens = caplens.to(device)\n",
    "        # Forward prop.\n",
    "        enc_image,  global_features = encoder(imgs)\n",
    "        predictions, alphas, betas, encoded_captions, decode_lengths, sort_ind = decoder(enc_image, global_features, \n",
    "                                                                                         caps, caplens)\n",
    "        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
    "        targets = encoded_captions[:, 1:]\n",
    "        # Remove timesteps that we didn't decode at, or are pads\n",
    "        # pack_padded_sequence is an easy trick to do this\n",
    "        scores, _ = pack_padded_sequence(predictions, decode_lengths, batch_first=True)\n",
    "        targets, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
    "        # Calculate loss\n",
    "        loss = criterion(scores, targets)\n",
    "        # Back prop.\n",
    "        decoder_optimizer.zero_grad()\n",
    "        if encoder_optimizer is not None:\n",
    "            encoder_optimizer.zero_grad() \n",
    "            \n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        decoder_optimizer.step()\n",
    "        if encoder_optimizer is not None:\n",
    "            encoder_optimizer.step()\n",
    "        # Keep track of metrics\n",
    "        top5 = accuracy(scores, targets, 5)\n",
    "        losses.update(loss.item(), sum(decode_lengths))    \n",
    "        top5accs.update(top5, sum(decode_lengths))\n",
    "        # Print status every print_freq iterations --> (print_freq * batch_size) images\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(epoch, i, len(train_loader),\n",
    "                                                                            loss=losses,\n",
    "                                                                            top5=top5accs))\n",
    "\n",
    "def validate(val_loader, encoder, decoder, beam_size, epoch, vocab_size):\n",
    "    \"\"\"\n",
    "    Funtion to validate over the complete dataset\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    results = []\n",
    "\n",
    "    for i, (img, image_id) in enumerate(tqdm(val_loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size))):\n",
    "\n",
    "        k = beam_size\n",
    "        infinite_pred = False\n",
    "        \n",
    "        # Encode\n",
    "        image = img.to(device)       # (1, 3, 224, 224)\n",
    "        enc_image, global_features = encoder(image) # enc_image of shape (1,num_pixels,features)\n",
    "        # Flatten encoding\n",
    "        num_pixels = enc_image.size(1)\n",
    "        encoder_dim = enc_image.size(2)\n",
    "        # We'll treat the problem as having a batch size of k\n",
    "        enc_image = enc_image.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
    "        # Tensor to store top k previous words at each step; now they're just <start>\n",
    "        k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n",
    "        # Tensor to store top k sequences; now they're just <start>\n",
    "        seqs = k_prev_words  # (k, 1)\n",
    "        # Tensor to store top k sequences' scores; now they're just 0\n",
    "        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
    "        # Lists to store completed sequences, their alphas and scores\n",
    "        complete_seqs = list()\n",
    "        complete_seqs_scores = list()\n",
    "        # Start decoding\n",
    "        step = 1\n",
    "        h, c = decoder.init_hidden_state(enc_image)\n",
    "        spatial_image = F.relu(decoder.encoded_to_hidden(enc_image))  # (k,num_pixels,hidden_size)\n",
    "        global_image = F.relu(decoder.global_features(global_features))      # (1,embed_dim)\n",
    "        # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
    "        while True:\n",
    "            embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (k,embed_dim)\n",
    "            inputs = torch.cat((embeddings, global_image.expand_as(embeddings)), dim = 1)    \n",
    "            h, c, st = decoder.LSTM(inputs , (h, c))  # (batch_size_t, hidden_size)\n",
    "            # Run the adaptive attention model\n",
    "            out_l, _, _ = decoder.adaptive_attention(spatial_image, h, st)\n",
    "            # Compute the probability over the vocabulary\n",
    "            scores = decoder.fc(out_l)      # (batch_size, vocab_size)\n",
    "            scores = F.log_softmax(scores, dim=1)   # (s, vocab_size)\n",
    "            # (k,1) will be (k,vocab_size), then (k,vocab_size) + (s,vocab_size) --> (s, vocab_size)\n",
    "            scores = top_k_scores.expand_as(scores) + scores  \n",
    "            # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
    "            if step == 1:\n",
    "                #Remember: torch.topk returns the top k scores in the first argument, and their respective indices in the second argument\n",
    "                top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
    "            else:\n",
    "                # Unroll and find top scores, and their unrolled indices\n",
    "                top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
    "\n",
    "            # Convert unrolled indices to actual indices of scores\n",
    "            prev_word_inds = top_k_words / vocab_size  # (s) \n",
    "            next_word_inds = top_k_words % vocab_size  # (s) \n",
    "            # Add new words to sequences, alphas\n",
    "            seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
    "            # Which sequences are incomplete (didn't reach <end>)?\n",
    "            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if next_word != word_map['<end>']]\n",
    "            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "            # Set aside complete sequences\n",
    "            if len(complete_inds) > 0:\n",
    "                complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "                complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "            k -= len(complete_inds)  # reduce beam length accordingly\n",
    "\n",
    "            if k == 0:\n",
    "                break\n",
    "\n",
    "            # Proceed with incomplete sequences\n",
    "            seqs = seqs[incomplete_inds]              \n",
    "            h = h[prev_word_inds[incomplete_inds]]\n",
    "            c = c[prev_word_inds[incomplete_inds]]\n",
    "            spatial_image = spatial_image[prev_word_inds[incomplete_inds]]\n",
    "            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "            # Break if things have been going on too long\n",
    "            if step > 50:\n",
    "                infinite_pred = True\n",
    "                break\n",
    "\n",
    "            step += 1\n",
    "            \n",
    "        if infinite_pred is not True:\n",
    "            i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "            seq = complete_seqs[i]\n",
    "        else:\n",
    "            seq = seqs[0][:20]\n",
    "            seq = [seq[i].item() for i in range(len(seq))]\n",
    "                \n",
    "        # Construct Sentence\n",
    "        sen_idx = [w for w in seq if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}]\n",
    "        sentence = ' '.join([rev_word_map[sen_idx[i]] for i in range(len(sen_idx))])\n",
    "        item_dict = {\"image_id\": image_id.item(), \"caption\": sentence}\n",
    "        results.append(item_dict)\n",
    "    \n",
    "    print(\"Calculating Evalaution Metric Scores......\\n\")\n",
    "    \n",
    "    resFile = 'cococaptioncider/results/captions_val2014_results_' + str(epoch) + '.json' \n",
    "    evalFile = 'cococaptioncider/results/captions_val2014_eval_' + str(epoch) + '.json' \n",
    "    # Calculate Evaluation Scores\n",
    "    with open(resFile, 'w') as wr:\n",
    "        json.dump(results,wr)\n",
    "        \n",
    "    coco = COCO(annFile)\n",
    "    cocoRes = coco.loadRes(resFile)\n",
    "    # create cocoEval object by taking coco and cocoRes\n",
    "    cocoEval = COCOEvalCap(coco, cocoRes)\n",
    "    # evaluate on a subset of images\n",
    "    # please remove this line when evaluating the full validation set\n",
    "    cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "    # evaluate results\n",
    "    cocoEval.evaluate()    \n",
    "    # Save Scores for all images in resFile\n",
    "    with open(evalFile, 'w') as w:\n",
    "        json.dump(cocoEval.eval, w)\n",
    "\n",
    "    return cocoEval.eval['CIDEr'], cocoEval.eval['Bleu_4']\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    with open('caption data/WORDMAP_coco.json', 'r') as j:\n",
    "        word_map = json.load(j)\n",
    "\n",
    "    rev_word_map = {v: k for k, v in word_map.items()}  # idx2word\n",
    "\n",
    "    if checkpoint is None:\n",
    "        decoder = DecoderWithAttention(hidden_size = hidden_size,\n",
    "                                       vocab_size = len(word_map), \n",
    "                                       att_dim = attention_dim, \n",
    "                                       embed_size = emb_dim,\n",
    "                                       encoded_dim = 2048) \n",
    "\n",
    "        encoder = Encoder(hidden_size = hidden_size, embed_size = emb_dim)\n",
    "        decoder_optimizer = torch.optim.Adam(params=decoder.parameters(),lr=decoder_lr, betas = (0.8,0.999))\n",
    "        encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
    "                                             lr=encoder_lr, betas = (0.8,0.999)) if fine_tune_encoder else None\n",
    "\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
    "        best_cider = checkpoint['cider']\n",
    "        decoder = checkpoint['decoder']\n",
    "        decoder_optimizer = checkpoint['decoder_optimizer']\n",
    "        encoder = checkpoint['encoder']\n",
    "        encoder_optimizer = checkpoint['encoder_optimizer']\n",
    "        if fine_tune_encoder is True and encoder_optimizer is None:\n",
    "            encoder.fine_tune(fine_tune_encoder)\n",
    "            encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),lr=encoder_lr)\n",
    "            print(\"Finetuning the CNN\")\n",
    "\n",
    "    # Move to GPU, if available\n",
    "    decoder = decoder.to(device)\n",
    "    encoder = encoder.to(device)\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    \n",
    "    # TODO change transforms\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    \n",
    "    # TODO change train loader\n",
    "    train_loader = torch.utils.data.DataLoader(COCOTrainDataset(transform=transforms.Compose([normalize])),\n",
    "                                               batch_size = batch_size, \n",
    "                                               shuffle=True, \n",
    "                                               pin_memory=True)\n",
    "    \n",
    "    # TODO change val loader\n",
    "    val_loader = torch.utils.data.DataLoader(COCOValidationDataset(transform=transforms.Compose([normalize])),\n",
    "                                             batch_size = 1,\n",
    "                                             shuffle=True, \n",
    "                                             pin_memory=True)\n",
    "\n",
    "    # Epochs\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "\n",
    "        # Terminate training if there is no improvmenet for 8 epochs\n",
    "        if epochs_since_improvement == 8:\n",
    "            print(\"No Improvement for the last 6 epochs. Training Terminated\")\n",
    "            break\n",
    "\n",
    "        # Decay the learning rate by 0.8 every 3 epochs\n",
    "        if epoch % 3 == 0 and epoch !=0:\n",
    "            adjust_learning_rate(decoder_optimizer, 0.8)\n",
    "\n",
    "        # One epoch's training\n",
    "        train(train_loader=train_loader,\n",
    "              encoder=encoder,\n",
    "              decoder=decoder,\n",
    "              criterion=criterion,\n",
    "              encoder_optimizer=encoder_optimizer,\n",
    "              decoder_optimizer=decoder_optimizer,\n",
    "              epoch=epoch,\n",
    "              vocab_size = len(word_map))\n",
    "\n",
    "        # One epoch's validation\n",
    "        recent_cider, recent_bleu4 = validate(val_loader = val_loader, \n",
    "                                              encoder = encoder, \n",
    "                                              decoder = decoder,\n",
    "                                              beam_size = 3, \n",
    "                                              epoch = epoch, \n",
    "                                              vocab_size = len(word_map))\n",
    "\n",
    "        print(\"Epoch {}:\\tCIDEr Score: {}\\tBLEU-4 Score: {}\".format(epoch, recent_cider, recent_bleu4))\n",
    "\n",
    "        # Check if there was an improvement\n",
    "        is_best = recent_cider > best_cider\n",
    "        best_cider = max(recent_cider, best_cider)\n",
    "        if not is_best:\n",
    "            epochs_since_improvement += 1\n",
    "            print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
    "        else:\n",
    "            epochs_since_improvement = 0\n",
    "\n",
    "        save_checkpoint(epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, decoder_optimizer, recent_cider, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy",
   "language": "python",
   "name": "spacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
