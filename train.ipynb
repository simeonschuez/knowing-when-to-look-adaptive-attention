{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from cococaption.pycocotools.coco import COCO\n",
    "from cococaption.pycocoevalcap.eval import COCOEvalCap\n",
    "import torch.backends.cudnn as cudnn\n",
    "from models.captioning_models import *\n",
    "from util import *\n",
    "\n",
    "from data_utils import get_karpathy_split\n",
    "from data_loader_captions import get_caption_loader\n",
    "from build_vocab import Vocabulary\n",
    "from tqdm.autonotebook import tqdm\n",
    "from string import punctuation\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "from data_loader_captions import filename_from_id\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#Model Parameters\n",
    "emb_dim = 512                  # dimension of word embeddings\n",
    "attention_dim = 512            # attention hidden size\n",
    "hidden_size = 512              # dimension of decoder RNN\n",
    "cudnn.benchmark = True         # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
    "# Training parameters\n",
    "start_epoch = 0\n",
    "epochs = 40                             # number of epochs to train before finetuning the encoder. Set to 18 when finetuning ecoder\n",
    "epochs_since_improvement = 0            # keeps track of number of epochs since there's been an improvement in validation BLEU\n",
    "batch_size = 80                         # set to 32 when finetuning the encoder\n",
    "workers = 1                             # number of workers for data-loading\n",
    "encoder_lr = 1e-4                       # learning rate for encoder. if fine-tuning, change to 1e-5 for CNN parameters only\n",
    "decoder_lr = 5e-4                       # learning rate for decoder\n",
    "grad_clip = 0.1                         # clip gradients at an absolute value of\n",
    "best_cider = 0.                         # Current BLEU-4 score \n",
    "print_freq = 1# 100                        # print training/validation stats every __ batches\n",
    "fine_tune_encoder = False                # set to true after 20 epochs \n",
    "checkpoint = None    # path to checkpoint, None at the begining\n",
    "annFile = '/home/vu48pok/Dokumente/Projekte/reg/knowing-when-to-look-adaptive-attention/cococaption/annotations/captions_val2014.json'  # Location of validation annotations\n",
    "\n",
    "splits_path = '/home/vu48pok/.data/compling/data/corpora/external/MSCOCO/COCO/splits/karpathy/caption_datasets/'\n",
    "caps_path = '/home/vu48pok/.data/compling/data/corpora/external/MSCOCO/COCO/'\n",
    "image_dir = '/home/vu48pok/.data/compling/data/corpora/external/MSCOCO/COCO/'\n",
    "crop_size=224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/vu48pok/Dokumente/Projekte/reg/knowing-when-to-look-adaptive-attention/data/coco_vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch, vocab_size):\n",
    "\n",
    "    decoder.train()                 # train mode (dropout and batchnorm is used)\n",
    "    encoder.train()\n",
    "    losses = AverageMeter()         # loss (per decoded word)\n",
    "    top5accs = AverageMeter()       # top5 accuracy\n",
    "\n",
    "    # Batches\n",
    "    for i, (imgs, caps, caplens) in enumerate(train_loader):\n",
    "\n",
    "        # Move to GPU, if available\n",
    "        imgs = imgs.to(device)\n",
    "        caps = caps.to(device)\n",
    "        caplens = caplens.to(device)\n",
    "        # Forward prop.\n",
    "        enc_image,  global_features = encoder(imgs)\n",
    "        predictions, alphas, betas, encoded_captions, decode_lengths, sort_ind = decoder(enc_image, global_features, \n",
    "                                                                                         caps, caplens)\n",
    "        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
    "        targets = encoded_captions[:, 1:]\n",
    "        # Remove timesteps that we didn't decode at, or are pads\n",
    "        # pack_padded_sequence is an easy trick to do this\n",
    "        \n",
    "        scores, _, _, _ = pack_padded_sequence(predictions, decode_lengths, batch_first=True)\n",
    "        targets, _, _, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
    "        # Calculate loss\n",
    "        loss = criterion(scores, targets)\n",
    "        # Back prop.\n",
    "        decoder_optimizer.zero_grad()\n",
    "        if encoder_optimizer is not None:\n",
    "            encoder_optimizer.zero_grad() \n",
    "            \n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        decoder_optimizer.step()\n",
    "        if encoder_optimizer is not None:\n",
    "            encoder_optimizer.step()\n",
    "        # Keep track of metrics\n",
    "        top5 = accuracy(scores, targets, 5)\n",
    "        losses.update(loss.item(), sum(decode_lengths))    \n",
    "        top5accs.update(top5, sum(decode_lengths))\n",
    "        # Print status every print_freq iterations --> (print_freq * batch_size) images\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(epoch, i, len(train_loader),\n",
    "                                                                            loss=losses,\n",
    "                                                                            top5=top5accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/local/.anaconda/envs/spacy/lib/python3.6/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.container.Sequential' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/mnt/local/.anaconda/envs/spacy/lib/python3.6/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/mnt/local/.anaconda/envs/spacy/lib/python3.6/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.batchnorm.BatchNorm2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/mnt/local/.anaconda/envs/spacy/lib/python3.6/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/mnt/local/.anaconda/envs/spacy/lib/python3.6/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.MaxPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/mnt/local/.anaconda/envs/spacy/lib/python3.6/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torchvision.models.resnet.Bottleneck' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/mnt/local/.anaconda/envs/spacy/lib/python3.6/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.pooling.AvgPool2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/mnt/local/.anaconda/envs/spacy/lib/python3.6/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/mnt/local/.anaconda/envs/spacy/lib/python3.6/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.LSTMCell' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/mnt/local/.anaconda/envs/spacy/lib/python3.6/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/mnt/local/.anaconda/envs/spacy/lib/python3.6/site-packages/torch/serialization.py:593: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('/home/vu48pok/.data/compling/data/misc/models/AdaptiveAttention/BEST_checkpoint_12.pth.tar', map_location=torch.device('cpu'))\n",
    "\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
    "best_cider = checkpoint['cider']\n",
    "decoder = checkpoint['decoder']\n",
    "decoder_optimizer = checkpoint['decoder_optimizer']\n",
    "encoder = checkpoint['encoder']\n",
    "encoder_optimizer = checkpoint['encoder_optimizer']\n",
    "if fine_tune_encoder is True and encoder_optimizer is None:\n",
    "    encoder.fine_tune(fine_tune_encoder)\n",
    "    encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),lr=encoder_lr)\n",
    "    print(\"Finetuning the CNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_df = get_karpathy_split(splits_path=splits_path, caps_path=caps_path)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((crop_size, crop_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5387d09f6c64e80b5b36a11d5fe1b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'AvgPool2d' object has no attribute 'divisor_override'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-fd36f41acb96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaps_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-86f791dd4d1e>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(caps_df, encoder, decoder, epoch, vocab, max_len)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0menc_image\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mglobal_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mnum_pix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/local/.anaconda/envs/spacy/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dokumente/Projekte/reg/knowing-when-to-look-adaptive-attention/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mnum_pixels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mencoded_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Get the global features of the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mglobal_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (batch_size, 2048)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0menc_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#  (batch_size,7,7,2048)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0menc_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_pixels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m          \u001b[0;31m# (batch_size,num_pixels,2048)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/local/.anaconda/envs/spacy/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/local/.anaconda/envs/spacy/lib/python3.6/site-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         return F.avg_pool2d(input, self.kernel_size, self.stride,\n\u001b[0;32m--> 554\u001b[0;31m                             self.padding, self.ceil_mode, self.count_include_pad, self.divisor_override)\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/local/.anaconda/envs/spacy/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 576\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AvgPool2d' object has no attribute 'divisor_override'"
     ]
    }
   ],
   "source": [
    "validate(caps_df, encoder, decoder, 0, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(caps_df, encoder, decoder, epoch, vocab, max_len=20):\n",
    "    \"\"\"\n",
    "    Funtion to validate over the complete dataset\n",
    "    \"\"\"\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    results = []\n",
    "    \n",
    "    val_ids = np.unique(caps_df.loc[caps_df.split == 'val'].image_id)\n",
    "    for i, image_id in tqdm(enumerate(val_ids[:100])):\n",
    "\n",
    "        entry = caps_df.loc[caps_df.image_id == image_id].iloc[0]\n",
    "        # get image filename\n",
    "        img_path = os.path.join(\n",
    "            image_dir, '{coco_split}2014/',\n",
    "            filename_from_id(image_id, prefix='COCO_{coco_split}2014_')\n",
    "            )\n",
    "        img_path = img_path.format(coco_split=entry.coco_split)\n",
    "        # read image file and transform\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        img = transform(image).unsqueeze(0)\n",
    "\n",
    "        enc_image,  global_features = encoder(img)\n",
    "        num_pix = enc_image.shape[1]\n",
    "\n",
    "        spatial_image = F.relu(decoder.encoded_to_hidden(enc_image))  # (batch_size,num_pixels,hidden_size)\n",
    "        global_image = F.relu(decoder.global_features(global_features))      # (batch_size,embed_size)\n",
    "\n",
    "        alphas_stored = torch.zeros(max_len, num_pix+1)\n",
    "        betas_stored = torch.zeros(max_len,1)\n",
    "        pred = torch.LongTensor([[vocab('<start>')]]).to(device)   # (1, 1)  \n",
    "        betas_stored = torch.zeros(max_len,1)\n",
    "\n",
    "        h,c = decoder.init_hidden_state(enc_image)                    #  (1,hidden_size)\n",
    "\n",
    "        for timestep in range(max_len):\n",
    "            embeddings = decoder.embedding(pred).squeeze(1)       # (1,1,embed_dim) --> (1,embed_dim)    \n",
    "            inputs = torch.cat((embeddings,global_image), dim = 1)    # (1, embed_dim * 2)\n",
    "            h, c, st = decoder.LSTM(inputs, (h, c))  # (1, hidden_size)\n",
    "            # Run the adaptive attention model\n",
    "            out, alpha, beta = decoder.adaptive_attention(spatial_image, h, st)\n",
    "            # Compute the probability\n",
    "            pt = decoder.fc(out)  \n",
    "            _,pred = pt.max(1)\n",
    "            sampled.append(pred.item())\n",
    "            alphas_stored[timestep] = alpha\n",
    "            betas_stored[timestep] = beta.item()\n",
    "\n",
    "        generated_words = [vocab.idx2word[sampled[i]] for i in range(len(sampled))]\n",
    "        sentence = ' '.join([word for word in generated_words if word != '<end>'])\n",
    "        \n",
    "        item_dict = {\"image_id\": image_id.item(), \"caption\": sentence}\n",
    "        results.append(item_dict)\n",
    "        \n",
    "    print(\"Calculating Evalaution Metric Scores......\\n\")\n",
    "    \n",
    "    resFile = 'results/captions_val2014_results_' + str(epoch) + '.json' \n",
    "    evalFile = 'results/captions_val2014_eval_' + str(epoch) + '.json' \n",
    "    # Calculate Evaluation Scores\n",
    "    with open(resFile, 'w') as wr:\n",
    "        json.dump(results,wr)\n",
    "        \n",
    "    coco = COCO(annFile)\n",
    "    cocoRes = coco.loadRes(resFile)\n",
    "    # create cocoEval object by taking coco and cocoRes\n",
    "    cocoEval = COCOEvalCap(coco, cocoRes)\n",
    "    # evaluate on a subset of images\n",
    "    # please remove this line when evaluating the full validation set\n",
    "    cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "    # evaluate results\n",
    "    cocoEval.evaluate()    \n",
    "    # Save Scores for all images in resFile\n",
    "    with open(evalFile, 'w') as w:\n",
    "        json.dump(cocoEval.eval, w)\n",
    "\n",
    "    return cocoEval.eval['CIDEr'], cocoEval.eval['Bleu_4']       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "with open('caption data/WORDMAP_coco.json', 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "\n",
    "rev_word_map = {v: k for k, v in word_map.items()}  # idx2word\n",
    "\n",
    "if checkpoint is None:\n",
    "    decoder = DecoderWithAttention(hidden_size = hidden_size,\n",
    "                                   vocab_size = len(word_map), \n",
    "                                   att_dim = attention_dim, \n",
    "                                   embed_size = emb_dim,\n",
    "                                   encoded_dim = 2048) \n",
    "\n",
    "    encoder = Encoder(hidden_size = hidden_size, embed_size = emb_dim)\n",
    "    decoder_optimizer = torch.optim.Adam(params=decoder.parameters(),lr=decoder_lr, betas = (0.8,0.999))\n",
    "    encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
    "                                         lr=encoder_lr, betas = (0.8,0.999)) if fine_tune_encoder else None\n",
    "\n",
    "else:\n",
    "    checkpoint = torch.load(checkpoint)\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
    "    best_cider = checkpoint['cider']\n",
    "    decoder = checkpoint['decoder']\n",
    "    decoder_optimizer = checkpoint['decoder_optimizer']\n",
    "    encoder = checkpoint['encoder']\n",
    "    encoder_optimizer = checkpoint['encoder_optimizer']\n",
    "    if fine_tune_encoder is True and encoder_optimizer is None:\n",
    "        encoder.fine_tune(fine_tune_encoder)\n",
    "        encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),lr=encoder_lr)\n",
    "        print(\"Finetuning the CNN\")\n",
    "\n",
    "# Move to GPU, if available\n",
    "decoder = decoder.to(device)\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((crop_size, crop_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "caps_df = get_karpathy_split(splits_path=splits_path, caps_path=caps_path)\n",
    "\n",
    "train_loader = get_caption_loader(\n",
    "    decoding_level='word', \n",
    "    split='train',\n",
    "    data_df=caps_df.loc[caps_df.split == 'train'].iloc[:10000], \n",
    "    image_dir=image_dir, \n",
    "    vocab=vocab,\n",
    "    transform=transform, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False,\n",
    "    num_workers=2, \n",
    "    drop_last=False   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epochs\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    \n",
    "    if epoch > 2:\n",
    "        break\n",
    "    \n",
    "\n",
    "    # Terminate training if there is no improvmenet for 8 epochs\n",
    "    if epochs_since_improvement == 8:\n",
    "        print(\"No Improvement for the last 6 epochs. Training Terminated\")\n",
    "        break\n",
    "\n",
    "    # Decay the learning rate by 0.8 every 3 epochs\n",
    "    if epoch % 3 == 0 and epoch !=0:\n",
    "        adjust_learning_rate(decoder_optimizer, 0.8)\n",
    "\n",
    "    # One epoch's training\n",
    "    train(train_loader=train_loader,\n",
    "          encoder=encoder,\n",
    "          decoder=decoder,\n",
    "          criterion=criterion,\n",
    "          encoder_optimizer=encoder_optimizer,\n",
    "          decoder_optimizer=decoder_optimizer,\n",
    "          epoch=epoch,\n",
    "          vocab_size = len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a kitchen with a toilet and a toilet a man is in a kitchen with a toilet and a toilet a man is in a kitchen with a toilet and a toilet a man is in a kitchen with a toilet and a toilet a man is in a kitchen with a toilet and a toilet\n"
     ]
    }
   ],
   "source": [
    "val_ids = np.unique(caps_df.loc[caps_df.split == 'val'].image_id)\n",
    "\n",
    "image_id = val_ids[2]\n",
    "\n",
    "entry = caps_df.loc[caps_df.image_id == image_id].iloc[0]\n",
    "# get image filename\n",
    "img_path = os.path.join(\n",
    "    image_dir, '{coco_split}2014/',\n",
    "    filename_from_id(image_id, prefix='COCO_{coco_split}2014_')\n",
    "    )\n",
    "img_path = img_path.format(coco_split=entry.coco_split)\n",
    "# read image file and transform\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "img = transform(image).unsqueeze(0)\n",
    "\n",
    "enc_image, global_features = encoder(img)\n",
    "num_pix = enc_image.shape[1]\n",
    "\n",
    "spatial_image = F.relu(decoder.encoded_to_hidden(enc_image))  # (batch_size,num_pixels,hidden_size)\n",
    "global_image = F.relu(decoder.global_features(global_features))      # (batch_size,embed_size)\n",
    "\n",
    "alphas_stored = torch.zeros(max_len, num_pix+1)\n",
    "betas_stored = torch.zeros(max_len,1)\n",
    "pred = torch.LongTensor([[vocab('<start>')]]).to(device)   # (1, 1)  \n",
    "betas_stored = torch.zeros(max_len,1)\n",
    "\n",
    "h,c = decoder.init_hidden_state(enc_image)                    #  (1,hidden_size)\n",
    "\n",
    "for timestep in range(max_len):\n",
    "    embeddings = decoder.embedding(pred).squeeze(1)       # (1,1,embed_dim) --> (1,embed_dim)    \n",
    "    inputs = torch.cat((embeddings,global_image), dim = 1)    # (1, embed_dim * 2)\n",
    "    h, c, st = decoder.LSTM(inputs, (h, c))  # (1, hidden_size)\n",
    "    # Run the adaptive attention model\n",
    "    out, alpha, beta = decoder.adaptive_attention(spatial_image, h, st)\n",
    "    # Compute the probability\n",
    "    pt = decoder.fc(out)  \n",
    "    _,pred = pt.max(1)\n",
    "    sampled.append(pred.item())\n",
    "    alphas_stored[timestep] = alpha\n",
    "    betas_stored[timestep] = beta.item()\n",
    "\n",
    "generated_words = [vocab.idx2word[sampled[i]] for i in range(len(sampled))]\n",
    "filtered_words = ' '.join([word for word in generated_words if word != '<end>'])\n",
    "\n",
    "print(filtered_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
